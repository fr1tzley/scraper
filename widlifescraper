import requests
from bs4 import BeautifulSoup
import pandas as pd

rows = []

def scrapelist(list):
    count1 = 0

    cookies1 = {
        'ASP.NET_SessionId': 'czsvw0uwujenegtpvxrcjlkn',
    }

    headers1 = {
        'authority': 'forestsclearance.nic.in',
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'en-US,en;q=0.9',
        'cache-control': 'max-age=0',
        # 'cookie': 'ASP.NET_SessionId=czsvw0uwujenegtpvxrcjlkn',
        'sec-ch-ua': '"Google Chrome";v="119", "Chromium";v="119", "Not?A_Brand";v="24"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'none',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    }
    
    while count1 < len(list):
        url = list[count1]
        page = requests.get(
            url,
            cookies=cookies,
            headers=headers,
            verify=False)

        soup = BeautifulSoup(page.content, "html.parser")

        results = soup.find_all("span", {"class": "li2"})

        count2 = 0
        row_dict = {}


        for result in results:
            splitdata = result.text.split(": ")

            if len(splitdata) < 2:
                continue

            key = splitdata[0].strip()
            value = splitdata[1].strip()
            
            if "Unnamed" in key:
                continue
            row_dict[key] = value
            count2 += 1
        
        tables = soup.find_all("table", {"class": "ez1"})

        for table in tables:
            table_rows = table.find_all('tr')
            if len(table_rows) >= 3:
                title = table_rows[0].string
                header_row = table_rows[1].find_all("th")
                data_rows = table_rows[2:]
                row_dict[title] = " "

                header_strings = [*map(lambda arg : arg.text.strip(), header_row)]
                header_strings.pop(0)


                datacount = 0
                datastr = ""
                for data_row in data_rows:
                    data_strings = [*map(lambda arg: arg.text.strip(), data_row.find_all("td"))]
                    data_strings.pop(0)

                    for hs, ds in zip(header_strings, data_strings):
                        row_dict[hs + "(" + title.strip() + ")" + datastr] = ds

                    datacount = datacount + 1
                    datastr = str(datacount)

        rows.append(row_dict)
        count1 += 1
    

baseurl = "https://forestsclearance.nic.in/"

cookies = {
    'ASP.NET_SessionId': '2kknivruxtqap1wvwstlkzhg',
    'Path': '/',
}

headers = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    'Accept-Language': 'en-US,en;q=0.9',
    'Connection': 'keep-alive',
    # 'Cookie': 'ASP.NET_SessionId=2kknivruxtqap1wvwstlkzhg; Path=/',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'none',
    'Sec-Fetch-User': '?1',
    'Upgrade-Insecure-Requests': '1',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    'sec-ch-ua': '"Google Chrome";v="119", "Chromium";v="119", "Not?A_Brand";v="24"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Windows"',
}

params = {
    'pids': 'Accepted_W',
    'st': 'new',
    'state': '14',
    'year': '-1',
    'cat': '-1',
}

response = requests.get('https://parivesh.nic.in/Wildnew_Online_Status.aspx', params=params, cookies=cookies, headers=headers)

soup = BeautifulSoup(response.content, "html.parser")

results = soup.find_all("a", {"title": "click on for Viewing Report of PartI"})
hreflist = []
for res in results:
    href = res.attrs["href"][2:]
    hreflist.append("ht" + href)

scrapelist(hreflist)


pdata = pd.DataFrame(rows)
pdata.to_csv("data/newdata.csv")